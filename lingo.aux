\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Schleicher:1853}
\citation{dUrville:1832}
\citation{field2002linguistic}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction.}{1}{section.1}}
\citation{Waegele:DMP}
\citation{Bourlat:06}
\citation{Rouse:16}
\citation{irwin:2005}
\citation{heggarty:2010,franccois2015trees}
\citation{Greenhill:09}
\citation{Huson:10}
\citation{Bowern:10}
\citation{Hudson:91,Griffiths:97,Arenas:13,Rasmussen:14}
\citation{Swadesh:52}
\citation{Sankoff:73}
\citation{Lee:11,Gray:09,Dunn:11}
\citation{Bouckaert:12}
\citation{Dunn:05}
\citation{Albu:06,Cysouw:08,Wichmann:07,Wichmann:10}
\citation{Nichols:92}
\citation{Longobardi:09}
\citation{Heggarty:00}
\citation{BouchardCote:13}
\citation{Gell-Mann:11,Dunn:11}
\citation{Dediu:13}
\@writefile{toc}{\contentsline {section}{\numberline {2}Lexicostatistics and language phylogenetics.}{4}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Modern work flows for sound data.}{4}{section.3}}
\citation{Campbell:1998,Crowley:2010}
\citation{Campbell:1998,Crowley:2010}
\citation{IDS}
\citation{Buck:49}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison of workflows to study language evolution. The comparative method and the sound change methods are explained in detail in the following sections, where we show how those methods produce relevant and reliable data. We assume that the traditional Swadesh Method is so well known that it does not need to be recapitulated in detail \citep  {Campbell:1998, Crowley:2010}. Early computational approaches were mainly `black box methods', and merely provided reasonable phylogenetic trees without a deeper understanding of the evolutionary events and processes explaining the phylogenies they predicted.}}{5}{figure.1}}
\newlabel{fig:MCyTalk}{{1}{5}{Comparison of workflows to study language evolution. The comparative method and the sound change methods are explained in detail in the following sections, where we show how those methods produce relevant and reliable data. We assume that the traditional Swadesh Method is so well known that it does not need to be recapitulated in detail \cite {Campbell:1998, Crowley:2010}. Early computational approaches were mainly \QUOTE {black box methods}, and merely provided reasonable phylogenetic trees without a deeper understanding of the evolutionary events and processes explaining the phylogenies they predicted}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}IDS as an example of word list data.}{5}{subsection.3.1}}
\citation{Calude:2011}
\citation{Pagel:2013}
\citation{Calude:2011,Calude:2014}
\citation{Haspelmath:2009}
\citation{Steiner:11a,Hruschka:14}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Identifying similarities.}{6}{subsection.3.2}}
\newlabel{sec:scoring}{{3.2}{6}{Identifying similarities}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Word alignments and their scoring.}{6}{subsubsection*.1}}
\citation{Kondrak:00}
\citation{Cysouw:07}
\citation{Kondrak:09}
\citation{Steiner:11a}
\citation{Notredame:07}
\citation{Steiner:11a}
\citation{Heggarty:00}
\citation{Steiner:11a}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A hypothetical language tree for 5 (proto-)languages (dark blue) and one (proto-)sound (pink). Greek letter correspond to reconstructed proto-languages and proto-sounds and Latin letters to the observed sounds and languages. Sound change laws (typically used by linguists, turquoise) and scoring models (used in quantitative methods, dark green) describe both changes between languages in sounds/characters. However, while sound change laws describe the changes in the sound system from the ancestral language to the descending language, scoring models describe the cumulative effect of the sound changes in the languages under investigation. For example, the scoring model (dashed-dotted lines) would state that $a$ from language $A$ is often found at those positions in the words where language $B$ has sound $b$. It thus indirectly imply that $a$ and $b$ originate from the same proto-sound. The sound change laws (straight lines, i.e. branches of the language tree) would state that there is a proto-sound ${}^*\delta $ which changed into $a$ in language $A$ and $b$ in language $B$.}}{8}{figure.2}}
\newlabel{fig:ScoringModels}{{2}{8}{A hypothetical language tree for 5 (proto-)languages (dark blue) and one (proto-)sound (pink). Greek letter correspond to reconstructed proto-languages and proto-sounds and Latin letters to the observed sounds and languages. Sound change laws (typically used by linguists, turquoise) and scoring models (used in quantitative methods, dark green) describe both changes between languages in sounds/characters. However, while sound change laws describe the changes in the sound system from the ancestral language to the descending language, scoring models describe the cumulative effect of the sound changes in the languages under investigation. For example, the scoring model (dashed-dotted lines) would state that $a$ from language $A$ is often found at those positions in the words where language $B$ has sound $b$. It thus indirectly imply that $a$ and $b$ originate from the same proto-sound. The sound change laws (straight lines, i.e. branches of the language tree) would state that there is a proto-sound ${}^*\delta $ which changed into $a$ in language $A$ and $b$ in language $B$}{figure.2}{}}
\citation{Durbin:98,Altschul:10}
\citation{Kondrak:00,Cysouw:07,Kondrak:09,Steiner:11a}
\citation{needleman:70}
\newlabel{eq:score}{{1}{9}{Word alignments and their scoring}{equation.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Example alignment for words from Mocov{\'\i } and Pilag{\'a} representing the concept `world'. Each bigram pair in the alignment is scored separately, resulting in a total score $\sigma =17.06$ in this example. Thus the normalized score is $\textit  {nscore}=17.06/(8-2)=2.84$.}}{10}{figure.3}}
\newlabel{fig:aln}{{3}{10}{Example alignment for words from Mocov{\'\i } and Pilag{\'a} representing the concept \QUOTE {world}. Each bigram pair in the alignment is scored separately, resulting in a total score $\sigma =17.06$ in this example. Thus the normalized score is $\textit {nscore}=17.06/(8-2)=2.84$}{figure.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{From alignment scores to cognates.}{10}{subsubsection*.2}}
\citation{Steiner:11a}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Distribution of pairwise alignment score for 11 Indo-European (Table\nobreakspace  {}\ref  {tab:bigramscores}) (left) and 3 Guaycuruan languages (Toba, Pilag{\'a}, Mocov{\'\i }) (right) with a simple unigram scoring model (left in each panel) and a bigram scoring model (right in each panel) trained on the initial alignments obtained with the simple unigram scoring model.}}{11}{figure.4}}
\newlabel{fig:unibigramdist}{{4}{11}{Distribution of pairwise alignment score for 11 Indo-European (Table~\ref {tab:bigramscores}) (left) and 3 Guaycuruan languages (Toba, Pilag{\'a}, Mocov{\'\i }) (right) with a simple unigram scoring model (left in each panel) and a bigram scoring model (right in each panel) trained on the initial alignments obtained with the simple unigram scoring model}{figure.4}{}}
\citation{Begleiter:04}
\citation{Hruschka:14}
\@writefile{toc}{\contentsline {subsubsection}{A probabilistic model of lexical evolution.}{12}{subsubsection*.3}}
\newlabel{sec:tanmethod}{{3.2}{12}{A probabilistic model of lexical evolution}{subsubsection*.3}{}}
\citation{Pennisi:05,Liberles:07}
\@writefile{toc}{\contentsline {subsubsection}{Comparing the two approaches.}{13}{subsubsection*.4}}
\citation{Covington:96,Kondrak:03}
\citation{Waegele:DMP}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Aligning words using a likelihood based method can be used to detect cognates. Left: Data for 4 Swadesh meanings (all, ash, bark, belly) across 20 Turkic languages were aligned and the best alignment scores for each pair of words were converted to a distance measure and shown as a heatmap: black pixels: small distance between words (between $0$ and $2$), white pixels: large distance between words (${}>14$), gray pixels: intermediate distances between words (between $2$ and $14$). Islands of darker pixels along the diagonal reflect cognate classes. Right: An ROC curve for detecting cognates within 50 meaning classes in Turkic shows it is possible to find an alignment score cutoff with high sensitivity and specificity in separating cognates from non-cognates (Area-under-the-curve AUC$=0.90$). Note that although only a small detail of the entire data set, namely the first four meanings sorted by their English description, is shown on the left, the ROC uses the entire dataset. }}{14}{figure.5}}
\newlabel{fig:DanROC}{{5}{14}{Aligning words using a likelihood based method can be used to detect cognates. Left: Data for 4 Swadesh meanings (all, ash, bark, belly) across 20 Turkic languages were aligned and the best alignment scores for each pair of words were converted to a distance measure and shown as a heatmap: black pixels: small distance between words (between $0$ and $2$), white pixels: large distance between words (${}>14$), gray pixels: intermediate distances between words (between $2$ and $14$). Islands of darker pixels along the diagonal reflect cognate classes. Right: An ROC curve for detecting cognates within 50 meaning classes in Turkic shows it is possible to find an alignment score cutoff with high sensitivity and specificity in separating cognates from non-cognates (Area-under-the-curve AUC$=0.90$). Note that although only a small detail of the entire data set, namely the first four meanings sorted by their English description, is shown on the left, the ROC uses the entire dataset}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Phylolinguistics.}{14}{subsection.3.3}}
\newlabel{Sect:SimReg}{{3.3}{14}{Phylolinguistics}{subsection.3.3}{}}
\citation{Pennisi:05}
\citation{Liberles:07}
\citation{Bowern:12a,Bowern:12b}
\@writefile{toc}{\contentsline {subsubsection}{Trees and their reconstruction.}{15}{subsubsection*.5}}
\citation{Kondrak:00}
\citation{BouchardCote:13}
\citation{Hruschka:14}
\citation{Steiner:11a}
\citation{Pagel_rates}
\citation{Bromham:15}
\@writefile{toc}{\contentsline {subsubsection}{Beyond computational biology.}{16}{subsubsection*.6}}
\citation{Kingman:82}
\citation{WhorfTrager:37,Haas:69}
\citation{Hockett:58,Greenberg:87,Fox:95}
\@writefile{toc}{\contentsline {subsubsection}{Deep phylogeny and its limitations.}{17}{subsubsection*.7}}
\citation{Waegele:DMP}
\citation{Zwickl:02}
\citation{Dufraigne:05}
\@writefile{toc}{\contentsline {subsubsection}{The effects of borrowing.}{18}{subsubsection*.8}}
\citation{Steiner:11a}
\citation{Steiner:11a}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Distribution of pairwise alignment scores for languages from the Mataco (Nivacl{\'e}, Maca, Wich{\'\i }, Chorote) and Guaycuruan (Toba, Pilag{\'a}, Mocov{\'\i }) languages. In panels (a-c), pairs of words with the same meaning according to the input word lists are compared with scores for pairs of words with independently randomized assignments to meanings. For comparisons within each language family (panels a and b) a clear shift to the right is observed for meaning-matched word pairs: that is, more meaning-matched pairs with large scores are observed compared to pairs with randomized meanings. In contrast, the two distributions are nearly identical for comparisons of one Mataco with one Guaycuruan language. Thus, there is a clear statistical signal for relatedness within Mataco and within Guaycuruan, but not between the Mataco and Guaycuruan families. (d) In a more detailed analysis, each pair of languages is compared separately and the statistical support for the difference of distribution is measured by the Kolmogorov-Smirnov test. We find strongly supported blocks (cooler colours) within the Mataco and Guaycuruan language families, but little signal across the family division. Data were recomputed from\nobreakspace  {}\citet  {Steiner:11a}. }}{19}{figure.6}}
\newlabel{fig:permut}{{6}{19}{Distribution of pairwise alignment scores for languages from the Mataco (Nivacl{\'e}, Maca, Wich{\'\i }, Chorote) and Guaycuruan (Toba, Pilag{\'a}, Mocov{\'\i }) languages. In panels (a-c), pairs of words with the same meaning according to the input word lists are compared with scores for pairs of words with independently randomized assignments to meanings. For comparisons within each language family (panels a and b) a clear shift to the right is observed for meaning-matched word pairs: \ie {} more meaning-matched pairs with large scores are observed compared to pairs with randomized meanings. In contrast, the two distributions are nearly identical for comparisons of one Mataco with one Guaycuruan language. Thus, there is a clear statistical signal for relatedness within Mataco and within Guaycuruan, but not between the Mataco and Guaycuruan families. (d) In a more detailed analysis, each pair of languages is compared separately and the statistical support for the difference of distribution is measured by the Kolmogorov-Smirnov test. We find strongly supported blocks (cooler colours) within the Mataco and Guaycuruan language families, but little signal across the family division. Data were recomputed from~\citet {Steiner:11a}}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}The virtue of statistical tests.}{19}{subsection.3.4}}
\citation{Baxter:00}
\citation{dingemanse2015}
\citation{Steiner:11a}
\citation{Smirnov:1948}
\citation{Bonferroni:1936}
\citation{Holm:1979,Hochberg:1988,Hommel:1988,Benjamini:1995,Storey:2002}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Estimates of the false discovery rate as a function of the predicted cognates for three pairs of languages. As the score cutoff $\theta $ decreases with the number of predictions, the FDR increases approximately monotonically. For the closely related Pilag{\'a} and Toba hundreds of cognates can be identified without much danger to make mistakes. Larger cognate sets for the related, but much more diverged languages Chorote and Nivacl{\'e} cannot be extracted without contamination by false positives, roughly 20\% within the top 100 predictions. For the presumably unrelated languages Chorote and Pilag{\'a}, the FDR increases even more rapidly.}}{22}{figure.7}}
\newlabel{fig:FDR}{{7}{22}{Estimates of the false discovery rate as a function of the predicted cognates for three pairs of languages. As the score cutoff $\theta $ decreases with the number of predictions, the FDR increases approximately monotonically. For the closely related Pilag{\'a} and Toba hundreds of cognates can be identified without much danger to make mistakes. Larger cognate sets for the related, but much more diverged languages Chorote and Nivacl{\'e} cannot be extracted without contamination by false positives, roughly 20\% within the top 100 predictions. For the presumably unrelated languages Chorote and Pilag{\'a}, the FDR increases even more rapidly}{figure.7}{}}
\citation{Doyle:11}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Comparison of 70 South American languages with English. The histogram shows the number $s-\delimiter "426830A s\delimiter "526930B $ of words that are more similar than the randomized baseline.}}{23}{figure.8}}
\newlabel{fig:Chist}{{8}{23}{Comparison of 70 South American languages with English. The histogram shows the number $s-\langle s\rangle $ of words that are more similar than the randomized baseline}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Quantifying regular change: the importance of representation.}{23}{subsection.3.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Overlap between the pairwise cognates: (a) Pilag{\'a} and Mocov{\'\i } and Pilag{\'a} and the re-encoded wordlist of Mocov{\'\i }. 97.2\% of the cognate pairs are found in independently of the encoding. Reducing the FDR to 0.001, 97.5\% of the pairs are found independently of the encoding. (b) Tsez Mokok (IPA-like and cyrillic transcription) and Tsez Sagadin (cyrillic transcription). }}{25}{figure.9}}
\newlabel{fig:encoding}{{9}{25}{Overlap between the pairwise cognates: (a) Pilag{\'a} and Mocov{\'\i } and Pilag{\'a} and the re-encoded wordlist of Mocov{\'\i }. 97.2\% of the cognate pairs are found in independently of the encoding. Reducing the FDR to 0.001, 97.5\% of the pairs are found independently of the encoding. (b) Tsez Mokok (IPA-like and cyrillic transcription) and Tsez Sagadin (cyrillic transcription)}{figure.9}{}}
\citation{Pagel:NatureLetter}
\citation{Cutler:2000}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}From non-parametric overviews to detailed probabilistic models.}{26}{subsection.3.6}}
\citation{Hruschka:14}
\citation{Thorne:98}
\citation{Penny:01}
\citation{Hruschka:14}
\citation{Starostin:10}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Computational methods and inference.}{27}{subsection.3.7}}
\citation{Bowern:12a}
\citation{Crowley:1981}
\citation{Semple:03}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8}Putting dates on language trees.}{28}{subsection.3.8}}
\citation{Heggarty:06}
\citation{Gray:11}
\citation{Gray:03}
\citation{Gray:09}
\citation{Swadesh:55}
\citation{Bergsland:62}
\citation{Ehret:00,Starostin:00,Starostin:13,Holman:11}
\citation{Hruschka:14}
\citation{Pagel_rates}
\citation{Embleton:86}
\citation{Sanderson:02}
\citation{Korber:00}
\citation{Nichols:08}
\citation{Nicholls:06,Starostin}
\citation{Croft:04,Koptjevskaja-Tamm:08}
\citation{Berlin:69,Dahl:85,Levinson:03,Majid:08,Croft:08}
\citation{Fox:95}
\citation{Sweetser:90,Traugott:02}
\citation{Brown:76,Brown:79,Witkowski:78,Wilkins:96}
\citation{Brown:83}
\citation{Viberg:83}
\citation{Evans:92}
\citation{Derrig:78}
\citation{Nichols:96}
\@writefile{toc}{\contentsline {section}{\numberline {4}Models of meaning.}{30}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Paralogous words.}{30}{subsection.4.1}}
\citation{IDS}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Distribution of normalized alignment scores for (left) Swedish and (right) Old Prussian. For alignment scores of Swedish words, a bimodal curve can be seen. The left hill is assumed to represent scores of unrelated words, while the right one mostly contains alignments of similar words. Therefore, the cutoff is set to $3.5$ as this score separates the two hills. The distribution of alignment scores in Old Prussian shows that the data here is rather sparse. Hence, no cutoff can be chosen and the dataset was removed from this experiment.}}{32}{figure.10}}
\newlabel{fig:cutoffChoice}{{10}{32}{Distribution of normalized alignment scores for (left) Swedish and (right) Old Prussian. For alignment scores of Swedish words, a bimodal curve can be seen. The left hill is assumed to represent scores of unrelated words, while the right one mostly contains alignments of similar words. Therefore, the cutoff is set to $3.5$ as this score separates the two hills. The distribution of alignment scores in Old Prussian shows that the data here is rather sparse. Hence, no cutoff can be chosen and the dataset was removed from this experiment}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Distribution of $\delta $ in Indo-European, Northern-Caucasian, and all other languages (see Table\nobreakspace  {}\ref  {tab:langDeltas}). Medians of the distributions are shown as filled circles. We did not observe $\delta $-values smaller than zero. If there would not be a signal, the $\delta $-value would distributed symmetrically around zero as, for example, in Fig.\nobreakspace  {}\ref  {fig:Chist}.}}{32}{figure.11}}
\newlabel{fig:deltas}{{11}{32}{Distribution of $\delta $ in Indo-European, Northern-Caucasian, and all other languages (see Table~\ref {tab:langDeltas}). Medians of the distributions are shown as filled circles. We did not observe $\delta $-values smaller than zero. If there would not be a signal, the $\delta $-value would distributed symmetrically around zero as, for example, in Fig.~\ref {fig:Chist}}{figure.11}{}}
\citation{Youn:16}
\citation{Youn:16}
\citation{Youn:16}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces A portion of the lexical semantic network presented in \citet  {Youn:16} showing the heterogeneous connectivity structure of the lexical semantic network. The nodes are Swadesh concepts linked if there exist languages with words polysemous between the two concepts. The links are weighted proportional to the number of such polysemies, and rare single polysemies are omitted for simplicity. The data are collected from bilingual dictionaries of eighty-one languages unrelated at the level of genus in the standard linguistic classification. As clearly shown in the diagram, some concepts like `water' or `earth' are involved in many polysemies, whereas others like `moon' in very few. Further analysis reveals that these concept form three almost disconnected clusters shown here using different colors.}}{33}{figure.12}}
\newlabel{fig:semanticnet}{{12}{33}{A portion of the lexical semantic network presented in \citet {Youn:16} showing the heterogeneous connectivity structure of the lexical semantic network. The nodes are Swadesh concepts linked if there exist languages with words polysemous between the two concepts. The links are weighted proportional to the number of such polysemies, and rare single polysemies are omitted for simplicity. The data are collected from bilingual dictionaries of eighty-one languages unrelated at the level of genus in the standard linguistic classification. As clearly shown in the diagram, some concepts like \QUOTE {water} or \QUOTE {earth} are involved in many polysemies, whereas others like \QUOTE {moon} in very few. Further analysis reveals that these concept form three almost disconnected clusters shown here using different colors}{figure.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Polysemy.}{33}{subsection.4.2}}
\citation{Zalizniak}
\citation{EHL}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces A model of how semantic shift takes place through intermediate polysemy and synonymy. Meanings, represented here by capital letters, are expressed by words represented by lowercase letters. Over time, polysemies (a word representing more than one meaning) and synonymies (a meaning represented by more than one word) are generated by additional word-meaning links being generated, whereas loss of word-meaning links leads to loss of polysemies and synonymies. The net result is often a word replacement in which a word with one meaning can shift to being used to express a different meaning. In the figure, the word-meaning associations that may be the focus of a study are depicted by solid lines, whereas dotted lines represent associations outside the study set that may not be directly observed. The possible paths arising from sense extension are labeled as \textit  {Poly} indicating development of a polysemy or \textit  {Syn} indicating development of a synonymy, and sense specialization leading to a word losing the sense \textit  {X} is indicated by \textit  {RepX}.}}{34}{figure.13}}
\newlabel{fig:state_process_model}{{13}{34}{A model of how semantic shift takes place through intermediate polysemy and synonymy. Meanings, represented here by capital letters, are expressed by words represented by lowercase letters. Over time, polysemies (a word representing more than one meaning) and synonymies (a meaning represented by more than one word) are generated by additional word-meaning links being generated, whereas loss of word-meaning links leads to loss of polysemies and synonymies. The net result is often a word replacement in which a word with one meaning can shift to being used to express a different meaning. In the figure, the word-meaning associations that may be the focus of a study are depicted by solid lines, whereas dotted lines represent associations outside the study set that may not be directly observed. The possible paths arising from sense extension are labeled as \textit {Poly} indicating development of a polysemy or \textit {Syn} indicating development of a synonymy, and sense specialization leading to a word losing the sense \textit {X} is indicated by \textit {RepX}}{figure.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Measuring the success of statistical methods.}{35}{section.5}}
\citation{Dunn:11}
\citation{Blasi:16a}
\citation{Turchin:2010}
\citation{Hruschka:14}
\citation{Frunza:08,Kondrak:09,Hall:11,Steiner:11a,rama2013two}
\citation{BouchardCote:13}
\citation{Gray:00,Gray:03,Pagel:NatureLetter}
\citation{Hruschka:14}
\citation{Griffiths:97,Arenas:13}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion.}{36}{section.6}}
\citation{needleman:70}
\citation{Levenshtein:66}
\citation{Gotoh:82}
\citation{Lawrence:93}
\@writefile{toc}{\contentsline {section}{Appendix a: bigram alignment algorithm}{39}{section*.9}}
\newlabel{AppA}{{6}{39}{}{section*.9}{}}
\@writefile{toc}{\contentsline {section}{Appendix b: tables of languages}{40}{section*.10}}
\newlabel{AppB}{{6}{40}{}{section*.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {B1}{\ignorespaces Languages used for calculating $\delta $ values in Figure\nobreakspace  {}\ref  {fig:deltas}.}}{40}{table.1}}
\newlabel{tab:langDeltas}{{B1}{40}{Languages used for calculating $\delta $ values in Figure~\ref {fig:deltas}}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {B2}{\ignorespaces Languages used in Section\nobreakspace  {}\ref  {sec:tanmethod}.}}{40}{table.2}}
\newlabel{tab:turkic}{{B2}{40}{Languages used in Section~\ref {sec:tanmethod}}{table.2}{}}
\bibstyle{language}
\bibdata{lingo,ourstuff}
\bibcite{Albu:06}{{1}{2006}{{Albu}}{{}}}
\bibcite{Altschul:10}{{2}{2010}{{Altschul et~al.}}{{Altschul, Wootton, Zaslavsky, and Yu}}}
\bibcite{Arenas:13}{{3}{2013}{{Arenas}}{{}}}
\bibcite{Pagel_rates}{{4}{2008}{{Atkinson et~al.}}{{Atkinson, Meade, Venditti, Greenhill, and Pagel}}}
\bibcite{Baxter:00}{{5}{2000}{{Baxter and Manaster-Ramer}}{{}}}
\bibcite{Begleiter:04}{{6}{2004}{{Begleiter et~al.}}{{Begleiter, El-Yaniv, and Yona}}}
\bibcite{Bergsland:62}{{7}{1962}{{Bergsland and Vogt}}{{}}}
\bibcite{Berlin:69}{{8}{1969}{{Berlin and Kay}}{{}}}
\bibcite{Blasi:16a}{{9}{2016}{{Blasi et~al.}}{{Blasi, Christiansen, Hammarstr{\"o}m, Stadler, and Wichmann}}}
\bibcite{Bonferroni:1936}{{10}{1936}{{Bonferroni}}{{}}}
\bibcite{BouchardCote:13}{{11}{2013}{{Bouchard-C{\^o}t{\'e} et~al.}}{{Bouchard-C{\^o}t{\'e}, Hall, Griffiths, and Klein}}}
\bibcite{Bouckaert:12}{{12}{2012}{{Bouckaert et~al.}}{{Bouckaert, Lemey, Dunn, Greenhill, Alekseyenko, Drummond, Gray, Suchard, and Atkinson}}}
\bibcite{Bourlat:06}{{13}{2006}{{Bourlat et~al.}}{{Bourlat, Juliusdottir, Lowe, Freeman, Aronowicz, Kirschner, Lander, Thorndyke, Nakano, Kohn, Heyland, Moroz, Copley, and Telford}}}
\bibcite{Bowern:10}{{14}{2010}{{Bowern}}{{}}}
\bibcite{Bowern:12a}{{15}{2012}{{Bowern}}{{}}}
\bibcite{Bowern:12b}{{16}{2012}{{Bowern and Atkinson}}{{}}}
\bibcite{Brown:76}{{17}{1976}{{Brown}}{{}}}
\bibcite{Brown:79}{{18}{1979}{{Brown}}{{}}}
\@writefile{lot}{\contentsline {table}{\numberline {B3}{\ignorespaces Languages used for calculation of uni- and bigram score distributions in Figure\nobreakspace  {}\ref  {fig:unibigramdist}.}}{41}{table.3}}
\newlabel{tab:bigramscores}{{B3}{41}{Languages used for calculation of uni- and bigram score distributions in Figure~\ref {fig:unibigramdist}}{table.3}{}}
\@writefile{toc}{\contentsline {section}{Bibliography}{41}{section*.11}}
\bibcite{Brown:83}{{19}{1983}{{Brown}}{{}}}
\bibcite{Buck:49}{{20}{1949}{{Buck}}{{}}}
\bibcite{Calude:2011}{{21}{2011}{{Calude and Pagel}}{{}}}
\bibcite{Calude:2014}{{22}{2014}{{Calude and Pagel}}{{}}}
\bibcite{Campbell:1998}{{23}{1998}{{Campbell}}{{}}}
\bibcite{Covington:96}{{24}{1996}{{Covington}}{{}}}
\bibcite{Croft:04}{{25}{2004}{{Croft and Cruse}}{{}}}
\bibcite{Croft:08}{{26}{2008}{{Croft and Poole}}{{}}}
\bibcite{Crowley:2010}{{27}{2010}{{Crowley and Bowern}}{{}}}
\bibcite{Crowley:1981}{{28}{1981}{{Crowley and Dixon}}{{}}}
\bibcite{Cutler:2000}{{29}{2000}{{Cutler et~al.}}{{Cutler, Sebasti\'an-Gall\'es, Soler-Vilageliu, and van Ooijen}}}
\bibcite{Cysouw:08}{{30}{2008}{{Cysouw et~al.}}{{Cysouw, Album, and Dress}}}
\bibcite{Cysouw:07}{{31}{2007}{{Cysouw and Jung}}{{}}}
\bibcite{Dahl:85}{{32}{1985}{{Dahl}}{{}}}
\bibcite{Dediu:13}{{33}{2015}{{Dediu}}{{}}}
\bibcite{Derrig:78}{{34}{1978}{{Derrig}}{{}}}
\bibcite{dingemanse2015}{{35}{2015}{{Dingemanse et~al.}}{{Dingemanse, Blasi, Lupyan, Christiansen, and Monaghan}}}
\bibcite{Doyle:11}{{36}{2011}{{Doyle and Csete}}{{}}}
\bibcite{Dufraigne:05}{{37}{2005}{{Dufraigne et~al.}}{{Dufraigne, Fertil, Lespinats, Giron, and Deschavanne}}}
\bibcite{dUrville:1832}{{38}{1832}{{Dumont~d'\ Urville}}{{}}}
\bibcite{Dunn:11}{{39}{2011}{{Dunn et~al.}}{{Dunn, Greenhill, Levinson, and Gray}}}
\bibcite{Dunn:05}{{40}{2005}{{Dunn et~al.}}{{Dunn, Terrill, Reesink, Foley, and Levinson}}}
\bibcite{Durbin:98}{{41}{1998}{{Durbin et~al.}}{{Durbin, Eddy, Krogh, and Mitchison}}}
\bibcite{EHL}{{42}{}{{EHL}}{{}}}
\bibcite{Ehret:00}{{43}{2000}{{Ehret}}{{}}}
\bibcite{Embleton:86}{{44}{1986}{{Embleton}}{{}}}
\bibcite{Evans:92}{{45}{1992}{{Evans}}{{}}}
\bibcite{field2002linguistic}{{46}{2002}{{Field}}{{}}}
\bibcite{Fox:95}{{47}{1995}{{Fox}}{{}}}
\bibcite{franccois2015trees}{{48}{2015}{{Fran{\c {c}}ois}}{{}}}
\bibcite{Frunza:08}{{49}{2008}{{Frunz{\u {a}}}}{{}}}
\bibcite{Gell-Mann:11}{{50}{2011}{{Gell-Mann and Ruhlen}}{{}}}
\bibcite{Gotoh:82}{{51}{1982}{{Gotoh}}{{}}}
\bibcite{Gray:09}{{52}{2009}{{Gray et~al.}}{{Gray, Drummond, and Greenhill}}}
\bibcite{Gray:03}{{53}{2003}{{Gray and Atkinson}}{{}}}
\bibcite{Gray:11}{{54}{2011}{{Gray et~al.}}{{Gray, Atkinson, and Greenhill}}}
\bibcite{Gray:00}{{55}{2000}{{Gray and Jordan}}{{}}}
\bibcite{Greenberg:87}{{56}{1987}{{Greenberg}}{{}}}
\bibcite{Greenhill:09}{{57}{2009}{{Greenhill et~al.}}{{Greenhill, Currie, and Gray}}}
\bibcite{Griffiths:97}{{58}{1997}{{Griffiths and Marjoram}}{{}}}
\bibcite{Haas:69}{{59}{1969}{{Haas}}{{}}}
\bibcite{Hall:11}{{60}{2011}{{Hall and Klein}}{{}}}
\bibcite{Haspelmath:2009}{{61}{2009}{{Haspelmath}}{{}}}
\bibcite{Heggarty:00}{{62}{2000}{{Heggarty}}{{}}}
\bibcite{Heggarty:06}{{63}{2006}{{Heggarty}}{{}}}
\bibcite{heggarty:2010}{{64}{2010}{{Heggarty et~al.}}{{Heggarty, Maguire, and McMahon}}}
\bibcite{Hochberg:1988}{{65}{1988}{{Hochberg}}{{}}}
\bibcite{Hockett:58}{{66}{1958}{{Hockett}}{{}}}
\bibcite{Holm:1979}{{67}{1979}{{Holm}}{{}}}
\bibcite{Holman:11}{{68}{2011}{{Holman et~al.}}{{Holman, Brown, Wichmann, M{\"u}ller, Velupillai, Hammarstr{\"o}m, Sauppe, Jung, Bakker, Brown, Belyaev, Urban, Mailhammer, List, and Egorov}}}
\bibcite{Hommel:1988}{{69}{1988}{{Hommel}}{{}}}
\bibcite{Hruschka:14}{{70}{2015}{{Hruschka et~al.}}{{Hruschka, Branford, Smith, Wilkins, Meade, Pagel, and Bhattacharya}}}
\bibcite{Hudson:91}{{71}{1991}{{Hudson}}{{}}}
\bibcite{Huson:10}{{72}{2010}{{Huson et~al.}}{{Huson, Rupp, and Scornavacca}}}
\bibcite{irwin:2005}{{73}{2005}{{Irwin et~al.}}{{Irwin, Bensch, Irwin, and Price}}}
\bibcite{IDS}{{74}{2007}{{Key and Comrie}}{{}}}
\bibcite{Kondrak:00}{{75}{2000}{{Kondrak}}{{}}}
\bibcite{Kondrak:03}{{76}{2003}{{Kondrak}}{{}}}
\bibcite{Kondrak:09}{{77}{2009}{{Kondrak}}{{}}}
\bibcite{Koptjevskaja-Tamm:08}{{78}{2008}{{Koptjevskaja-Tamm}}{{}}}
\bibcite{Korber:00}{{79}{2000}{{Korber et~al.}}{{Korber, Muldoon, Theiler, Gao, Gupta, Lapedes, Hahn, Wolinsky, and Bhattacharya}}}
\bibcite{Lawrence:93}{{80}{1993}{{Lawrence et~al.}}{{Lawrence, Altschul, Boguski, Liu, Neuwald, and Wootton}}}
\bibcite{Lee:11}{{81}{2011}{{Lee and Hasegawa}}{{}}}
\bibcite{Levenshtein:66}{{82}{1966}{{Levenshtein}}{{}}}
\bibcite{Levinson:03}{{83}{2003}{{Levinson and Meira}}{{}}}
\bibcite{Liberles:07}{{84}{2007}{{Liberles}}{{}}}
\bibcite{Longobardi:09}{{85}{2009}{{Longobardi and Guardiano}}{{}}}
\bibcite{Majid:08}{{86}{2008}{{Majid et~al.}}{{Majid, Boster, and Bowerman}}}
\bibcite{needleman:70}{{87}{1970}{{Needleman and Wunsch}}{{}}}
\bibcite{Nicholls:06}{{88}{2006}{{Nicholls and Gray}}{{}}}
\bibcite{Nichols:92}{{89}{1992}{{Nichols}}{{}}}
\bibcite{Nichols:96}{{90}{1996}{{Nichols}}{{}}}
\bibcite{Nichols:08}{{91}{2008}{{Nichols and Warnow}}{{}}}
\bibcite{Notredame:07}{{92}{2007}{{Notredame}}{{}}}
\bibcite{Pagel:NatureLetter}{{93}{2007}{{Pagel et~al.}}{{Pagel, Atkinson, and Meade}}}
\bibcite{Pagel:2013}{{94}{2013}{{Pagel et~al.}}{{Pagel, Atkinson, Calude, and Meade}}}
\bibcite{Pennisi:05}{{95}{2005}{{Pennisi}}{{}}}
\bibcite{Penny:01}{{96}{2001}{{Penny et~al.}}{{Penny, McComish, Charleston, and Hendy}}}
\bibcite{rama2013two}{{97}{2013}{{Rama et~al.}}{{Rama, Kolachina, and Kolachina}}}
\bibcite{Rasmussen:14}{{98}{2014}{{Rasmussen et~al.}}{{Rasmussen, Hubisz, Gronau, and Adam}}}
\bibcite{Rouse:16}{{99}{2016}{{Rouse et~al.}}{{Rouse, Wilson, Carvajal, and Vrijenhoek}}}
\bibcite{Sanderson:02}{{100}{2002}{{Sanderson}}{{}}}
\bibcite{Sankoff:73}{{101}{1973}{{Sankoff}}{{}}}
\bibcite{Schleicher:1853}{{102}{1853}{{Schleicher}}{{}}}
\bibcite{Semple:03}{{103}{2003}{{Semple and Steel}}{{}}}
\bibcite{Smirnov:1948}{{104}{1948}{{Smirnov}}{{}}}
\bibcite{Starostin:10}{{105}{2010}{{Starostin}}{{}}}
\bibcite{Starostin:13}{{106}{2013}{{Starostin}}{{}}}
\bibcite{Starostin:00}{{107}{2000}{{Starostin}}{{}}}
\bibcite{Starostin}{{108}{2007}{{Starostin}}{{}}}
\bibcite{Steiner:11a}{{109}{2011}{{Steiner et~al.}}{{Steiner, Stadler, and Cysouw}}}
\bibcite{Storey:2002}{{110}{2002}{{Storey}}{{}}}
\bibcite{Swadesh:52}{{111}{1952}{{Swadesh}}{{}}}
\bibcite{Swadesh:55}{{112}{1955}{{Swadesh}}{{}}}
\bibcite{Sweetser:90}{{113}{1990}{{Sweetser}}{{}}}
\bibcite{Thorne:98}{{114}{1998}{{Thorne et~al.}}{{Thorne, Kishino, and Painter}}}
\bibcite{Traugott:02}{{115}{2002}{{Traugott and Dasher}}{{}}}
\bibcite{Turchin:2010}{{116}{2010}{{Turchin et~al.}}{{Turchin, Peiros, and Gell-Mann}}}
\bibcite{Viberg:83}{{117}{1983}{{Viberg}}{{}}}
\bibcite{Waegele:DMP}{{118}{2014}{{Waegele and Bartholomaeus}}{{}}}
\bibcite{WhorfTrager:37}{{119}{1937}{{Whorf and Trager}}{{}}}
\bibcite{Wichmann:07}{{120}{2007}{{Wichman and Saunders}}{{}}}
\bibcite{Wichmann:10}{{121}{2010}{{Wichmann et~al.}}{{Wichmann, Holman, Bakker, and Brown}}}
\bibcite{Wilkins:96}{{122}{1996}{{Wilkins}}{{}}}
\bibcite{Witkowski:78}{{123}{1979}{{Witkowski and Brown}}{{}}}
\bibcite{Benjamini:1995}{{124}{1995}{{Yoav~Benjamini}}{{}}}
\bibcite{Youn:16}{{125}{2016}{{Youn et~al.}}{{Youn, Sutton, Smith, Moore, Wilkins, Maddieson, Croft, and Bhattacharya}}}
\bibcite{Zalizniak}{{126}{2008}{{Zalizniak}}{{}}}
\bibcite{Zwickl:02}{{127}{2002}{{Zwickl and Hillis}}{{}}}
